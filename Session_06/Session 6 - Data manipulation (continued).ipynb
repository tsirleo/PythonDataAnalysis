{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6 - Data manipulation (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session agenda\n",
    "1. Pandas: Series and DataFrame data structures.\n",
    "2. Querying data structures, indexing.\n",
    "3. Reshaping and pivoting of data sets.\n",
    "4. Performing operations on Series and DataFrames.\n",
    "5. Handling missing data.\n",
    "6. Merging DataFrames.\n",
    "7. Aggregating and transforming data by group or by functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "pandas is built on top of NumPy.Today we will see a lot of points, which will be reminescent of what we have seen in NumPy. A lot of things, which you can do with NumPy array you can also do with pandas data structures.\n",
    "\n",
    "The two basic data structures of pandas are Series and DataFrame data structures.\n",
    "\n",
    "A Series is a one-dimensional array-like object containing an array of data (of any NumPy data type) and an associated array of data labels, called its index.\n",
    "\n",
    "A DataFrame represents a tabular, spreadsheet-like data structure containing an ordered\n",
    "collection of columns, each of which can be a different value type (numeric,\n",
    "string, boolean, etc.). The DataFrame has both a row and column index; it can be\n",
    "thought of as a dict of Series (one for all sharing the same index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Series and DataFrames\n",
    "We will now check different examples of creating pandas data structures.\n",
    "\n",
    "A Series data structure can be created from basically any container type (e.g. list, tuple, dict, set and etc.)\n",
    "The usual ways to create DataFrames are summarized below:\n",
    "\n",
    "1. 2D ndarray - a matrix of data, passing optional row and column labels\n",
    "2. dict of arrays, lists, or tuples - each sequence becomes a column in the DataFrame. All sequences must be the same length.\n",
    "3. NumPy structured/record array - treated as the “dict of arrays” case\n",
    "4. dict of Series - each value becomes a column. Indexes from each Series are unioned together to form the result’s row index if no explicit index is passed.\n",
    "5. dict of dicts - each inner dict becomes a column. Keys are unioned to form the row index as in the “dict of Series” case.\n",
    "6. list of dicts or Series - each item becomes a row in the DataFrame. Union of dict keys or Series indexes become the DataFrame’s column labels\n",
    "7. list of lists or tuples - Treated as the “2D ndarray” case\n",
    "8. another DataFrame - the DataFrame’s indexes are used unless different ones are passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing import once for the entire notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    6\n",
      "2    7\n",
      "3    8\n",
      "4    9\n",
      "dtype: int64\n",
      "0.0    1\n",
      "0.2    4\n",
      "0.4    7\n",
      "0.6    2\n",
      "0.8    8\n",
      "dtype: int64\n",
      "a   -0.164558\n",
      "b    1.485664\n",
      "c    1.993509\n",
      "d   -0.611370\n",
      "e   -1.380486\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sample indexes to use\n",
    "i1 = np.arange(5)\n",
    "i2 = np.arange(0.,1.,.2)\n",
    "i3 = ['a','b','c','d','e']\n",
    "i4 = ['sample_1','sample_2','sample_3','sample_4','sample_5']\n",
    "\n",
    "# Creating Series\n",
    "print(Series((5,6,7,8,9),index=i1))\n",
    "print(Series([1,4,7,2,8], index=i2))\n",
    "print(Series(np.random.randn(5), index=i3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1   2   3   4\n",
      "0   0   1   2   3   4\n",
      "1   5   6   7   8   9\n",
      "2  10  11  12  13  14\n",
      "                 a         b         c         d         e\n",
      "sample_1 -1.257675 -0.103393  1.002839  0.264139 -0.847316\n",
      "sample_2 -1.903442 -0.405832  1.177500 -0.021255  1.761447\n",
      "sample_3  0.403693 -1.370022  0.693562  0.055196  2.367731\n",
      "sample_4  2.363010 -0.797972 -1.014689 -1.389512 -1.591996\n",
      "sample_5 -2.590517 -2.057744 -0.052783 -0.190236  0.430150\n",
      "   pop   state  year\n",
      "0  1.5    Ohio  2000\n",
      "1  1.7    Ohio  2001\n",
      "2  3.6    Ohio  2002\n",
      "3  2.4  Nevada  2001\n",
      "4  2.9  Nevada  2002\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrames\n",
    "print(DataFrame([np.arange(5),np.arange(5,10),np.arange(10,15)]))\n",
    "print(DataFrame(np.random.randn(5,5), index=i4, columns=i3))\n",
    "data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\n",
    "        'year': [2000, 2001, 2002, 2001, 2002],\n",
    "        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\n",
    "print(DataFrame(data))\n",
    "# Check out the table above other options of creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series and DataFrame indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2000\n",
      "1    2001\n",
      "2    2002\n",
      "3    2001\n",
      "4    2002\n",
      "Name: year, dtype: int64\n",
      "0    2000\n",
      "1    2001\n",
      "2    2002\n",
      "3    2001\n",
      "4    2002\n",
      "Name: year, dtype: int64\n",
      "   pop state  year\n",
      "1  1.7  Ohio  2001\n",
      "2  3.6  Ohio  2002\n",
      "pop       3.6\n",
      "state    Ohio\n",
      "year     2002\n",
      "Name: 2, dtype: object\n",
      "year\n",
      "2001\n"
     ]
    }
   ],
   "source": [
    "#Basic slicing and indexing\n",
    "df = DataFrame(data)\n",
    "print(df.year)\n",
    "print(df['year'])\n",
    "print(df[1:3])\n",
    "#.ix can also be used to index rows\n",
    "print(df.ix[2])\n",
    "#.columns can also be used to index columns\n",
    "print(df.columns[2])\n",
    "print(df.year[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1  1.210488  0.259335  1.179418 -0.244646 -0.918095\n",
      "sample_2  0.610912 -0.444640  1.443535 -0.480085  2.472845\n",
      "sample_3  0.007454  0.795321 -0.528711 -0.939383  0.545476\n",
      "sample_4  0.630215 -0.010681  0.958657 -0.598138 -1.111613\n",
      "sample_5 -0.234037  1.346291  1.394233  0.290787 -0.525443\n",
      "New index\n",
      "['sample_2', 'sample_3', 'sample_4', 'sample_5', 'sample_6']\n",
      "Reindexed DataFrame rows\n",
      "                 a         b         c         d         e\n",
      "sample_2  0.610912 -0.444640  1.443535 -0.480085  2.472845\n",
      "sample_3  0.007454  0.795321 -0.528711 -0.939383  0.545476\n",
      "sample_4  0.630215 -0.010681  0.958657 -0.598138 -1.111613\n",
      "sample_5 -0.234037  1.346291  1.394233  0.290787 -0.525443\n",
      "sample_6       NaN       NaN       NaN       NaN       NaN\n",
      "Other strategies for filled in values\n",
      "                 a         b         c         d         e\n",
      "sample_2  0.610912 -0.444640  1.443535 -0.480085  2.472845\n",
      "sample_3  0.007454  0.795321 -0.528711 -0.939383  0.545476\n",
      "sample_4  0.630215 -0.010681  0.958657 -0.598138 -1.111613\n",
      "sample_5 -0.234037  1.346291  1.394233  0.290787 -0.525443\n",
      "sample_6 -0.234037  1.346291  1.394233  0.290787 -0.525443\n",
      "Reindexing DataFrame columns\n",
      "                 b         c\n",
      "sample_1  0.259335  1.179418\n",
      "sample_2 -0.444640  1.443535\n",
      "sample_3  0.795321 -0.528711\n",
      "sample_4 -0.010681  0.958657\n",
      "sample_5  1.346291  1.394233\n"
     ]
    }
   ],
   "source": [
    "# Reindexing\n",
    "df1 = DataFrame(np.random.randn(5,5), columns=i3, index=i4)\n",
    "print(df1)\n",
    "i5 = i4[1:]\n",
    "i5.append('sample_6')\n",
    "print('New index')\n",
    "print(i5)\n",
    "print('Reindexed DataFrame rows')\n",
    "print(df1.reindex(i5))\n",
    "print('Other strategies for filled in values')\n",
    "print(df1.reindex(i5,method='ffill'))\n",
    "print('Reindexing DataFrame columns')\n",
    "print(df1.reindex(columns=i3[1:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492 -0.177390  3.275841\n",
      "sample_2 -2.165332 -1.074811  0.268541  0.159464 -1.498251\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n",
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492 -0.177390  3.275841\n",
      "sample_2 -2.165332 -1.074811  0.268541  0.159464 -1.498251\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "                 a         b\n",
      "sample_4 -0.136005 -1.292736\n",
      "sample_5  1.095454 -0.655771\n",
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492  1.000000  1.000000\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n",
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492  1.000000  1.000000\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python34\\lib\\site-packages\\pandas\\core\\indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "c:\\python34\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Most of other types of indexing, which we have used for NumPy arrays, can be applied to pandas data structures\n",
    "# But we have to track the index instances\n",
    "df1 = DataFrame(np.random.randn(5,5), columns=i3, index=i4)\n",
    "print(df1)\n",
    "#For index instances slicing includes the end element\n",
    "print(df1['sample_1':'sample_3'])\n",
    "print(df1[i3[:2]].ix[i4[3:]])\n",
    "df1.loc[:2,3:] = 1\n",
    "print(df1)\n",
    "#Create a copy\n",
    "df1[i3[:2]].ix[i4[3:]] = 0\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492  1.000000  1.000000\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n",
      "                 a         b         c         d         e\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "\n",
      "\n",
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492       NaN       NaN\n",
      "sample_2 -2.165332 -1.074811       NaN       NaN       NaN\n",
      "sample_3       NaN -0.298890       NaN -0.113057       NaN\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166       NaN\n",
      "sample_5       NaN -0.655771       NaN       NaN -0.346265\n"
     ]
    }
   ],
   "source": [
    "#Boolean indexing\n",
    "print(df1)\n",
    "#Need to convert to NumPy array from list\n",
    "npi4 = np.array(i4)\n",
    "print(df1[(npi4 == 'sample_2') | (npi4 == 'sample_4')])\n",
    "print('\\n')\n",
    "print(df1[df1 < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Indexing\n",
    "Hierarchical indexing is an important feature of pandas enabling you to have multiple\n",
    "(two or more) index levels on an axis. Somewhat abstractly, it provides a way for you\n",
    "to work with higher dimensional data in a lower dimensional form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  1    0.633544\n",
      "   2   -0.643101\n",
      "   3   -0.828501\n",
      "b  1   -0.427717\n",
      "   2   -0.642839\n",
      "   3    0.164110\n",
      "c  1   -0.576410\n",
      "   2   -0.087753\n",
      "d  2    0.149458\n",
      "   3   -0.202405\n",
      "dtype: float64\n",
      "MultiIndex(levels=[['a', 'b', 'c', 'd'], [1, 2, 3]],\n",
      "           labels=[[0, 0, 0, 1, 1, 1, 2, 2, 3, 3], [0, 1, 2, 0, 1, 2, 0, 1, 1, 2]])\n",
      "b  1   -0.427717\n",
      "   2   -0.642839\n",
      "   3    0.164110\n",
      "c  1   -0.576410\n",
      "   2   -0.087753\n",
      "dtype: float64\n",
      "a   -0.643101\n",
      "b   -0.642839\n",
      "c   -0.087753\n",
      "d    0.149458\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = Series(np.random.randn(10),\n",
    "              index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'],\n",
    "                     [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])\n",
    "\n",
    "print(data)\n",
    "print(data.index)\n",
    "print(data['b':'c'])\n",
    "print(data[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping and pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1         2         3\n",
      "a  0.633544 -0.643101 -0.828501\n",
      "b -0.427717 -0.642839  0.164110\n",
      "c -0.576410 -0.087753       NaN\n",
      "d       NaN  0.149458 -0.202405\n",
      "a  1    0.633544\n",
      "   2   -0.643101\n",
      "   3   -0.828501\n",
      "b  1   -0.427717\n",
      "   2   -0.642839\n",
      "   3    0.164110\n",
      "c  1   -0.576410\n",
      "   2   -0.087753\n",
      "d  2    0.149458\n",
      "   3   -0.202405\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Stack and unstack\n",
    "u_data = data.unstack()\n",
    "print(u_data)\n",
    "print(u_data.stack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mass  sample_1  sample_2  time\n",
      "0   m+0 -1.558887  0.511669     1\n",
      "1   m+1  0.618861 -0.339522     1\n",
      "2   m+2 -1.337418 -0.141539     1\n",
      "3   m+0  0.906607 -1.949280     2\n",
      "4   m+1  0.280068 -0.187556     2\n",
      "5   m+2 -1.308806 -0.197907     2\n",
      "6   m+0 -1.017520 -1.737900     3\n",
      "7   m+1  1.292099  0.339754     3\n",
      "8   m+2 -1.636276  1.218481     3\n",
      "9   m+0  0.525630 -0.498552     4\n",
      "10  m+1  0.745133  0.210890     4\n",
      "11  m+2 -0.959105 -0.087750     4\n"
     ]
    }
   ],
   "source": [
    "#Pivoting\n",
    "data2 = {'time':[1,1,1,2,2,2,3,3,3,4,4,4],\n",
    "        'mass':['m+0','m+1','m+2','m+0','m+1','m+2','m+0','m+1','m+2','m+0','m+1','m+2'],\n",
    "        'sample_1':np.random.randn(12),\n",
    "        'sample_2':np.random.randn(12)}\n",
    "df7 = DataFrame(data2)\n",
    "print(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mass       m+0       m+1       m+2\n",
      "time                              \n",
      "1    -1.558887  0.618861 -1.337418\n",
      "2     0.906607  0.280068 -1.308806\n",
      "3    -1.017520  1.292099 -1.636276\n",
      "4     0.525630  0.745133 -0.959105\n",
      "time         1         2         3         4\n",
      "mass                                        \n",
      "m+0   0.511669 -1.949280 -1.737900 -0.498552\n",
      "m+1  -0.339522 -0.187556  0.339754  0.210890\n",
      "m+2  -0.141539 -0.197907  1.218481 -0.087750\n",
      "      sample_1                      sample_2                    \n",
      "mass       m+0       m+1       m+2       m+0       m+1       m+2\n",
      "time                                                            \n",
      "1    -1.558887  0.618861 -1.337418  0.511669 -0.339522 -0.141539\n",
      "2     0.906607  0.280068 -1.308806 -1.949280 -0.187556 -0.197907\n",
      "3    -1.017520  1.292099 -1.636276 -1.737900  0.339754  1.218481\n",
      "4     0.525630  0.745133 -0.959105 -0.498552  0.210890 -0.087750\n"
     ]
    }
   ],
   "source": [
    "print(df7.pivot('time','mass','sample_1'))\n",
    "print(df7.pivot('mass','time','sample_2'))\n",
    "print(df7.pivot('time','mass'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and basic array manipulation\n",
    "pandas has some overhead compared to NumPy, but it is still delivers better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List comprehension:\n",
      "1000 loops, best of 3: 2.65 ms per loop\n",
      "Map:\n",
      "1000 loops, best of 3: 3.67 ms per loop\n",
      "NumPy:\n",
      "1000 loops, best of 3: 7.62 µs per loop\n",
      "pandas:\n",
      "1000 loops, best of 3: 120 µs per loop\n"
     ]
    }
   ],
   "source": [
    "size = 10000\n",
    "print('List comprehension:')\n",
    "test = np.arange(size)\n",
    "%timeit -n 1000 result = [x ** 2 for x in test]\n",
    "\n",
    "print('Map:')\n",
    "test = np.arange(size)\n",
    "%timeit -n 1000 result = list(map(lambda x: x ** 2, test))\n",
    "\n",
    "print('NumPy:')\n",
    "test = np.arange(size)\n",
    "%timeit -n 1000 result = test ** 2\n",
    "\n",
    "print('pandas:')\n",
    "s_test = Series(test)\n",
    "%timeit -n 1000 result = s_test ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal functions\n",
    "NumPy universal function work fine with pandas objects. pandas has also a couple of additional handy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492  1.000000  1.000000\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n",
      "                 a         b         c         d         e\n",
      "sample_1  0.568827  0.769200  0.292441  2.718282  2.718282\n",
      "sample_2  0.114712  0.341362  1.308054  2.718282  2.718282\n",
      "sample_3  1.676113  0.741641  1.296974  0.893100  1.680277\n",
      "sample_4  0.872838  0.274519  0.851578  0.045677  1.078225\n",
      "sample_5  2.990539  0.519042  4.345778  2.192269  0.707325\n",
      "a   -1.253584\n",
      "b   -3.584612\n",
      "c    0.607624\n",
      "d   -0.414287\n",
      "e    2.248010\n",
      "dtype: float64\n",
      "a   -0.250717\n",
      "b   -0.716922\n",
      "c    0.121525\n",
      "d   -0.082857\n",
      "e    0.449602\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Examples of universal function use\n",
    "print(df1)\n",
    "print(np.exp(df1))\n",
    "print(np.sum(df1))\n",
    "print(np.mean(df1))\n",
    "# ... just the others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1 -0.564178 -0.262405 -1.229492  1.000000  1.000000\n",
      "sample_2 -2.165332 -1.074811  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005 -1.292736 -0.160664 -3.086166  0.075316\n",
      "sample_5  1.095454 -0.655771  1.469205  0.784937 -0.346265\n",
      "a    3.260785\n",
      "b    1.030331\n",
      "c    2.698697\n",
      "d    4.086166\n",
      "e    1.346265\n",
      "dtype: float64\n",
      "            a         b         c         d         e\n",
      "min -2.165332 -1.292736 -1.229492 -3.086166 -0.346265\n",
      "max  1.095454 -0.262405  1.469205  1.000000  1.000000\n",
      "              a      b      c      d      e\n",
      "sample_1  -0.56  -0.26  -1.23   1.00   1.00\n",
      "sample_2  -2.17  -1.07   0.27   1.00   1.00\n",
      "sample_3   0.52  -0.30   0.26  -0.11   0.52\n",
      "sample_4  -0.14  -1.29  -0.16  -3.09   0.08\n",
      "sample_5   1.10  -0.66   1.47   0.78  -0.35\n"
     ]
    }
   ],
   "source": [
    "#Apply method\n",
    "print(df1)\n",
    "print(df1.apply(lambda x: x.max() - x.min()))\n",
    "\n",
    "def f(x):\n",
    "    return Series([x.min(), x.max()], index=['min','max'])\n",
    "print(df1.apply(f))\n",
    "# Element-wise apply\n",
    "print(df1.applymap(lambda x: '%.2f' % x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "Common methods, which deals with missing data:\n",
    "\n",
    "1. dropna - filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n",
    "2. fillna - fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\n",
    "3. isnull - return like-type object containing boolean values indicating which values are missing / NA.\n",
    "4. notnull - negation of isnull.\n",
    "\n",
    "There are a multitude of different strategies for handling missing data. The way that the data was collected and processed will usually dictate that strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a         b         c         d         e\n",
      "sample_1       NaN -0.262405       NaN  1.000000  1.000000\n",
      "sample_2       NaN       NaN  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 -0.298890  0.260034 -0.113057  0.518959\n",
      "sample_4 -0.136005       NaN -0.160664       NaN  0.075316\n",
      "sample_5  1.095454       NaN  1.469205  0.784937 -0.346265\n",
      "                 a        b         c         d         e\n",
      "sample_3  0.516477 -0.29889  0.260034 -0.113057  0.518959\n",
      "                 a         b         c         d   e\n",
      "sample_1 -0.564178       NaN -1.229492       NaN NaN\n",
      "sample_2 -2.165332 -1.074811       NaN       NaN NaN\n",
      "sample_3       NaN       NaN       NaN       NaN NaN\n",
      "sample_4       NaN -1.292736       NaN -3.086166 NaN\n",
      "sample_5       NaN -0.655771       NaN       NaN NaN\n",
      "                 a         b         c         d   e\n",
      "sample_1 -0.564178       NaN -1.229492       NaN NaN\n",
      "sample_2 -2.165332 -1.074811       NaN       NaN NaN\n",
      "sample_4       NaN -1.292736       NaN -3.086166 NaN\n",
      "sample_5       NaN -0.655771       NaN       NaN NaN\n",
      "                 a         b         c         d\n",
      "sample_1 -0.564178       NaN -1.229492       NaN\n",
      "sample_2 -2.165332 -1.074811       NaN       NaN\n",
      "sample_3       NaN       NaN       NaN       NaN\n",
      "sample_4       NaN -1.292736       NaN -3.086166\n",
      "sample_5       NaN -0.655771       NaN       NaN\n",
      "                 a   b         c         d         e\n",
      "sample_1       NaN NaN       NaN  1.000000  1.000000\n",
      "sample_2       NaN NaN  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 NaN  0.260034       NaN  0.518959\n",
      "sample_4       NaN NaN       NaN       NaN  0.075316\n",
      "sample_5  1.095454 NaN  1.469205  0.784937       NaN\n",
      "                 a   b         c         d         e\n",
      "sample_1       NaN NaN       NaN  1.000000  1.000000\n",
      "sample_2       NaN NaN  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 NaN  0.260034       NaN  0.518959\n",
      "sample_5  1.095454 NaN  1.469205  0.784937       NaN\n"
     ]
    }
   ],
   "source": [
    "#Examples of handling missing data\n",
    "#Filtering or cleaning\n",
    "df2 = df1[df1 > -0.5]\n",
    "print(df2)\n",
    "print(df2.dropna())\n",
    "df3 = df1[df1 < -0.5]\n",
    "print(df3)\n",
    "print(df3.dropna(how='all'))\n",
    "print(df3.dropna(how='all',axis=1))\n",
    "df4 = df1[df1 > 0]\n",
    "print(df4)\n",
    "print(df4.dropna(thresh = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 a   b         c         d         e\n",
      "sample_1       NaN NaN       NaN  1.000000  1.000000\n",
      "sample_2       NaN NaN  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 NaN  0.260034       NaN  0.518959\n",
      "sample_4       NaN NaN       NaN       NaN  0.075316\n",
      "sample_5  1.095454 NaN  1.469205  0.784937       NaN\n",
      "                 a    b         c         d         e\n",
      "sample_1  0.000000  0.0  0.000000  1.000000  1.000000\n",
      "sample_2  0.000000  0.0  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477  0.0  0.260034  0.000000  0.518959\n",
      "sample_4  0.000000  0.0  0.000000  0.000000  0.075316\n",
      "sample_5  1.095454  0.0  1.469205  0.784937  0.000000\n",
      "                 a   b         c         d         e\n",
      "sample_1       NaN NaN       NaN  1.000000  1.000000\n",
      "sample_2       NaN NaN  0.268541  1.000000  1.000000\n",
      "sample_3  0.516477 NaN  0.260034  1.000000  0.518959\n",
      "sample_4  0.516477 NaN  0.260034  1.000000  0.075316\n",
      "sample_5  1.095454 NaN  1.469205  0.784937  0.075316\n"
     ]
    }
   ],
   "source": [
    "#Filling in missing data\n",
    "print(df4)\n",
    "print(df4.fillna(0))\n",
    "print(df4.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Combining and Merging Data Sets\n",
    "Data contained in pandas objects can be combined together in a number of built-in\n",
    "ways:\n",
    "\n",
    "1. merge - connects rows in DataFrames based on one or more keys. This will be familiar to users of SQL or other relational databases, as it implements database join operations.\n",
    "2. concat - glues or stacks together objects along an axis.\n",
    "3. combine_first - instance method, which enables splicing together overlapping data to fill in missing values in one object with values from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data1 key  data2\n",
      "0      0   b      1\n",
      "1      1   b      1\n",
      "2      6   b      1\n",
      "3      2   a      0\n",
      "4      4   a      0\n",
      "5      5   a      0\n",
      "   data1 key  data2\n",
      "0      0   b      1\n",
      "1      1   b      1\n",
      "2      6   b      1\n",
      "3      2   a      0\n",
      "4      4   a      0\n",
      "5      5   a      0\n",
      "   data1 lkey  data2 rkey\n",
      "0      0    b      1    b\n",
      "1      1    b      1    b\n",
      "2      6    b      1    b\n",
      "3      2    a      0    a\n",
      "4      4    a      0    a\n",
      "5      5    a      0    a\n",
      "   data1 key  data2\n",
      "0    0.0   b    1.0\n",
      "1    1.0   b    1.0\n",
      "2    6.0   b    1.0\n",
      "3    2.0   a    0.0\n",
      "4    4.0   a    0.0\n",
      "5    5.0   a    0.0\n",
      "6    3.0   c    NaN\n",
      "7    NaN   d    2.0\n",
      "    data1 key  data2\n",
      "0       0   b    1.0\n",
      "1       0   b    3.0\n",
      "2       1   b    1.0\n",
      "3       1   b    3.0\n",
      "4       2   a    0.0\n",
      "5       2   a    2.0\n",
      "6       3   c    NaN\n",
      "7       4   a    0.0\n",
      "8       4   a    2.0\n",
      "9       5   b    1.0\n",
      "10      5   b    3.0\n",
      "   data1 key  data2\n",
      "0      0   b      1\n",
      "1      0   b      3\n",
      "2      1   b      1\n",
      "3      1   b      3\n",
      "4      5   b      1\n",
      "5      5   b      3\n",
      "6      2   a      0\n",
      "7      2   a      2\n",
      "8      4   a      0\n",
      "9      4   a      2\n"
     ]
    }
   ],
   "source": [
    "mf1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "mf2 = DataFrame({'key': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "\n",
    "print(pd.merge(mf1,mf2))\n",
    "print(pd.merge(mf1,mf2, on='key'))\n",
    "\n",
    "mf3 = DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'], 'data1': range(7)})\n",
    "mf4 = DataFrame({'rkey': ['a', 'b', 'd'], 'data2': range(3)})\n",
    "print(pd.merge(mf3, mf4, left_on='lkey', right_on='rkey'))\n",
    "\n",
    "print(pd.merge(mf1, mf2, how='outer'))\n",
    "\n",
    "mf5 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})\n",
    "mf6 = DataFrame({'key': ['a', 'b', 'a', 'b', 'd'], 'data2': range(5)})\n",
    "\n",
    "print(pd.merge(mf5, mf6, on='key', how='left'))\n",
    "print(pd.merge(mf5, mf6, how='inner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  key1 key2  lval  rval\n",
      "0  foo  one   1.0   4.0\n",
      "1  foo  one   1.0   5.0\n",
      "2  foo  two   2.0   NaN\n",
      "3  bar  one   3.0   6.0\n",
      "4  bar  two   NaN   7.0\n",
      "  key1 key2_x  lval key2_y  rval\n",
      "0  foo    one     1    one     4\n",
      "1  foo    one     1    one     5\n",
      "2  foo    two     2    one     4\n",
      "3  foo    two     2    one     5\n",
      "4  bar    one     3    one     6\n",
      "5  bar    one     3    two     7\n",
      "  key1 key2_left  lval key2_right  rval\n",
      "0  foo       one     1        one     4\n",
      "1  foo       one     1        one     5\n",
      "2  foo       two     2        one     4\n",
      "3  foo       two     2        one     5\n",
      "4  bar       one     3        one     6\n",
      "5  bar       one     3        two     7\n"
     ]
    }
   ],
   "source": [
    "left = DataFrame({'key1': ['foo', 'foo', 'bar'],\n",
    "                  'key2': ['one', 'two', 'one'],\n",
    "                  'lval': [1, 2, 3]})\n",
    "right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\n",
    "                   'key2': ['one', 'one', 'one', 'two'],\n",
    "                   'rval': [4, 5, 6, 7]})\n",
    "print(pd.merge(left, right, on=['key1', 'key2'], how='outer'))\n",
    "print(pd.merge(left, right, on='key1'))\n",
    "print(pd.merge(left, right, on='key1', suffixes=('_left', '_right')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging on index\n",
    "In some cases, the merge key or keys in a DataFrame will be found in its index. In this\n",
    "case, you can pass left_index=True or right_index=True (or both) to indicate that the\n",
    "index should be used as the merge key. Check this feature in the documantation or reference materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by mechanics\n",
    "Most group operation can be described by the split-apply-combine process.\n",
    "\n",
    "In the first stage of the process, data contained in a pandas object, whether a Series, DataFrame, or otherwise, is split into\n",
    "groups based on one or more keys that you provide. The splitting is performed on a\n",
    "particular axis of an object. For example, a DataFrame can be grouped on its rows\n",
    "(axis=0) or its columns (axis=1). Once this is done, a function is applied to each group,\n",
    "producing a new value. Finally, the results of all those function applications are combined\n",
    "into a result object. The form of the resulting object will usually depend on what’s\n",
    "being done to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      data1     data2 key1 key2\n",
      "0  0.211274 -0.357062    a  one\n",
      "1  0.186323 -0.968224    a  two\n",
      "2  0.177146  1.984687    b  one\n",
      "3 -0.316796  1.297375    b  two\n",
      "4 -1.419189 -0.410050    a  one\n",
      "<pandas.core.groupby.SeriesGroupBy object at 0x0776F350>\n",
      "key1\n",
      "a   -0.340531\n",
      "b   -0.069825\n",
      "Name: data1, dtype: float64\n",
      "key1  key2\n",
      "a     one    -0.603957\n",
      "      two     0.186323\n",
      "b     one     0.177146\n",
      "      two    -0.316796\n",
      "Name: data1, dtype: float64\n",
      "key2       one       two\n",
      "key1                    \n",
      "a    -0.603957  0.186323\n",
      "b     0.177146 -0.316796\n",
      "Iterating over groups:\n",
      "a       data1     data2 key1 key2\n",
      "0  0.211274 -0.357062    a  one\n",
      "1  0.186323 -0.968224    a  two\n",
      "4 -1.419189 -0.410050    a  one\n",
      "b       data1     data2 key1 key2\n",
      "2  0.177146  1.984687    b  one\n",
      "3 -0.316796  1.297375    b  two\n"
     ]
    }
   ],
   "source": [
    "#Examples\n",
    "gdf = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n",
    "                'key2' : ['one', 'two', 'one', 'two', 'one'],\n",
    "                'data1' : np.random.randn(5),\n",
    "                'data2' : np.random.randn(5)})\n",
    "\n",
    "print(gdf)\n",
    "grouped = gdf['data1'].groupby(gdf['key1'])\n",
    "print(grouped)\n",
    "print(grouped.mean())\n",
    "means = gdf['data1'].groupby([gdf['key1'], gdf['key2']]).mean()\n",
    "print(means)\n",
    "print(means.unstack())\n",
    "\n",
    "#iterating over groups\n",
    "print('Iterating over groups:')\n",
    "for name, group in gdf.groupby('key1'):\n",
    "    print(name, group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              data2\n",
      "key1 key2          \n",
      "a    one  -0.383556\n",
      "     two  -0.968224\n",
      "b    one   1.984687\n",
      "     two   1.297375\n"
     ]
    }
   ],
   "source": [
    "#Selecting columns\n",
    "g1 = gdf.groupby('key1')['data1']\n",
    "g2 = gdf.groupby('key1')[['data2']]\n",
    "#Equivalent to\n",
    "gg1 = gdf['data1'].groupby(gdf['key1'])\n",
    "gg2 = gdf[['data2']].groupby(gdf['key1'])\n",
    "\n",
    "#Usually we will be grouping only over a small number of columns\n",
    "print(gdf.groupby(['key1', 'key2'])[['data2']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               a         b         c         d         e\n",
      "Joe    -1.997377 -1.466217 -0.894391  0.101571  0.074373\n",
      "Steve  -0.781462 -1.122434 -0.272265 -0.320494 -0.187241\n",
      "Wes    -1.270656 -0.085921  1.094874  0.567917 -0.688809\n",
      "Jim     1.226688  0.363249 -0.634265  0.284112 -1.057895\n",
      "Travis -0.156271 -0.852929  0.550426  0.202380  1.139974\n",
      "            blue       red\n",
      "Joe    -0.792821 -3.389220\n",
      "Steve  -0.592759 -2.091137\n",
      "Wes     1.662791 -2.045386\n",
      "Jim    -0.350152  0.532043\n",
      "Travis  0.752806  0.130774\n"
     ]
    }
   ],
   "source": [
    "#Other options to groupby with\n",
    "#Grouping with dicts\n",
    "people = DataFrame(np.random.randn(5, 5),\n",
    "                   columns=['a', 'b', 'c', 'd', 'e'],\n",
    "                   index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "print(people)\n",
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue', 'd': 'blue', 'e': 'red', 'f' : 'orange'}\n",
    "by_column = people.groupby(mapping, axis=1)\n",
    "print(by_column.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          a         b         c         d         e\n",
      "3 -2.041345 -1.188888 -0.433782  0.953600 -1.672330\n",
      "5 -0.781462 -1.122434 -0.272265 -0.320494 -0.187241\n",
      "6 -0.156271 -0.852929  0.550426  0.202380  1.139974\n"
     ]
    }
   ],
   "source": [
    "#Grouping with function\n",
    "print(people.groupby(len).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              a         b         c         d         e\n",
      "3 one -1.997377 -1.466217 -0.894391  0.101571 -0.688809\n",
      "  two  1.226688  0.363249 -0.634265  0.284112 -1.057895\n",
      "5 one -0.781462 -1.122434 -0.272265 -0.320494 -0.187241\n",
      "6 two -0.156271 -0.852929  0.550426  0.202380  1.139974\n"
     ]
    }
   ],
   "source": [
    "#Mixing grouping types\n",
    "key_list = ['one', 'one', 'one', 'two', 'two']\n",
    "print(people.groupby([len, key_list]).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cty          US                            JP          \n",
      "tenor         1         3         5         1         3\n",
      "0      2.328130  0.904139 -1.223392 -1.134816 -2.799629\n",
      "1      1.157060  1.402950 -0.435362 -1.219580 -1.421968\n",
      "2      0.098276 -1.555883 -0.502759 -0.686335  0.777765\n",
      "3     -0.063604 -0.440680  0.968237  0.433864 -0.725542\n",
      "cty  JP  US\n",
      "0     2   3\n",
      "1     2   3\n",
      "2     2   3\n",
      "3     2   3\n"
     ]
    }
   ],
   "source": [
    "#Grouping by index levels\n",
    "columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],[1, 3, 5, 1, 3]], names=['cty', 'tenor'])\n",
    "hier_df = DataFrame(np.random.randn(4, 5), columns=columns)\n",
    "print(hier_df)\n",
    "print(hier_df.groupby(level='cty', axis=1).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "Data transformations, which produce scalar values from arrays. We have already seen some of them applied to GroupBy object. There are a lot of aggregations, which you can perform. Even create your own aggregations, just checkout aggregate or agg method. pandas also provide a numver of optimized aggregate methods:\n",
    "\n",
    "1. count - number of non-NA values in the group\n",
    "2. sum - sum of non-NA values\n",
    "3. mean - mean of non-NA values\n",
    "4. median - arithmetic median of non-NA values\n",
    "5. std, var - unbiased (n - 1 denominator) standard deviation and variance\n",
    "6. min, max - minimum and maximum of non-NA values\n",
    "7. prod - product of non-NA values\n",
    "8. first, last - first and last non-NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-wise operations and transformations\n",
    "Apart from doing aggregations, there are other operations, which you can do with groups. There are two methods, which allow us to do all sorts of group-wise operations. Checkout transfrom and apply methods.\n",
    "\n",
    "Apply is basically the most generic way of doing transfromations on the grouped by objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               a         b         c         d         e\n",
      "Joe    -1.997377 -1.466217 -0.894391  0.101571  0.074373\n",
      "Steve  -0.781462 -1.122434 -0.272265 -0.320494 -0.187241\n",
      "Wes    -1.270656 -0.085921  1.094874  0.567917 -0.688809\n",
      "Jim     1.226688  0.363249 -0.634265  0.284112 -1.057895\n",
      "Travis -0.156271 -0.852929  0.550426  0.202380  1.139974\n",
      "               a         b         c         d         e\n",
      "Joe    -1.141435 -0.801689  0.250303  0.290623  0.175179\n",
      "Steve   0.222613 -0.379592 -0.453265 -0.018191 -0.622568\n",
      "Wes    -1.141435 -0.801689  0.250303  0.290623  0.175179\n",
      "Jim     0.222613 -0.379592 -0.453265 -0.018191 -0.622568\n",
      "Travis -1.141435 -0.801689  0.250303  0.290623  0.175179\n",
      "               a         b         c         d         e\n",
      "Joe    -0.855942 -0.664528 -1.144694 -0.189052 -0.100806\n",
      "Steve  -1.004075 -0.742841  0.181000 -0.302303  0.435327\n",
      "Wes    -0.129221  0.715768  0.844571  0.277295 -0.863989\n",
      "Jim     1.004075  0.742841 -0.181000  0.302303 -0.435327\n",
      "Travis  0.985163 -0.051240  0.300123 -0.088242  0.964795\n",
      "                a             b             c             d             e\n",
      "one -1.480297e-16 -3.700743e-17  3.700743e-17 -1.850372e-17  3.700743e-17\n",
      "two  0.000000e+00  0.000000e+00  2.775558e-17  0.000000e+00  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "print(people)\n",
    "key = ['one', 'two', 'one', 'two', 'one']\n",
    "print(people.groupby(key).transform(np.mean))\n",
    "\n",
    "def demean(arr):\n",
    "    return arr - arr.mean()\n",
    "demeaned = people.groupby(key).transform(demean)\n",
    "print(demeaned)\n",
    "print(demeaned.groupby(key).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

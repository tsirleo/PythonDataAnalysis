{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Session agenda\n",
    "\n",
    "1. Types of features. Examples of feature extraction from texts, images and other objects\n",
    "2. Basics of sklearn.preprocessing module.\n",
    "3. Data standardization, normalization and binarization.\n",
    "4. Working with missing values.\n",
    "5. Polynomial features and custom transformations.\n",
    "6. Pipelining data analysis tasks with sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of features. Examples of feature extraction from texts, images and other objects\n",
    "sklearn library has a special module sklearn.feature_extraction, which deals with feature extraction tasks for typical cases of objects (e.g. texts and images).\n",
    "\n",
    "sklearn.feature_extraction module provides vectorizer and transformer classes, which adhere to the same basic API and allow to extract and transform features from different objects. These classes use NumPy arrays and SciPy sparse arrays to represent underlying data.\n",
    "\n",
    "This functionality is somewhat complimentary to the functionality of pandas. So you have to decide, which library to use for your data preprocessing and then if necessary perform the convertion from one data structure to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. 33.]\n",
      " [ 0.  1.  0. 12.]\n",
      " [ 0.  0.  1. 18.]]\n",
      "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Fransisco', 'temperature': 18.},\n",
    "]\n",
    "\n",
    "vector = DictVectorizer()\n",
    "\n",
    "print(vector.fit_transform(measurements).toarray())\n",
    "print(vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from texts.\n",
    "We will now see a couple of examples of how simple it can be to extract features from texts using sklearn.feature_extraction. We will use the typical models (e.g. bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "#Vectorizer has a very versitale constructor and have a lot of parameters, which can be configured\n",
    "#CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'text', 'document', 'to', 'analyze']\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(\"This is a text document to analyze.\"))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.vocabulary_.get('document'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizers also support creation of N-gram models.\n",
    "#Here is an example of combined 1-gram and 2-gram model.\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "print(analyze('Bi-grams are cool!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 2 1 1 1 0 1 0 0 0 1 1 0]\n",
      " [1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1]]\n",
      "  (3, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus)\n",
    "print(X_2.toarray())\n",
    "\n",
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "print(X_2[:, feature_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp']\n",
      "[[1 1 1 0 1 1 1 0]\n",
      " [1 1 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Character or symbol N-gram models can be created by specifing 'char_wb' analyzer.\n",
    "# Basically, you can create your own model by creating a special analyzer.\n",
    "digram_char_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=1)\n",
    "counts = digram_char_vectorizer.fit_transform(['words', 'wprds'])\n",
    "print(digram_char_vectorizer.get_feature_names())\n",
    "print(counts.toarray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf–idf term weighting\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \n",
    "$$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}.$$\n",
    "Using the TfidfTransformer‘s default settings, ```TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)``` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n",
    "$$\\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1,$$\n",
    "where $n_d$ is the total number of documents, and $\\text{df}(d,t)$ is the number of documents that contain term $t$. The resulting tf-idf vectors are then normalized by the Euclidean norm:\n",
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}.$$\n",
    "This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.\n",
    "The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn’s ```TfidfTransformer``` and ```TfidfVectorizer``` differ slightly from the standard textbook notation that defines the idf as\n",
    "$$\\text{idf}(t) = log{\\frac{n_d}{1+\\text{df}(d,t)}}.$$\n",
    "In the ```TfidfTransformer``` and ```TfidfVectorizer``` with ```smooth_idf=False```, the “1” count is added to the idf instead of the idf’s denominator:\n",
    "$$\\text{idf}(t) = log{\\frac{n_d}{\\text{df}(d,t)}} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81940995 0.         0.57320793]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [1.         0.         0.        ]\n",
      " [0.47330339 0.88089948 0.        ]\n",
      " [0.58149261 0.         0.81355169]]\n",
      "[1.         2.79175947 2.09861229]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "print(tfidf.toarray())\n",
    "print(transformer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_3 = vectorizer.fit_transform(corpus)\n",
    "print(X_3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hashing trick\n",
    "The above vectorization scheme is simple but the fact that it holds an in- memory mapping from the string tokens to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large datasets:\n",
    "\n",
    "1. The larger the corpus, the larger the vocabulary will grow and hence the memory use too, fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset.\n",
    "2. Building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner.\n",
    "3. Pickling and un-pickling vectorizers with a large vocabulary_ can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),\n",
    "4. It is not easily possible to split the vectorization work into concurrent sub tasks as the vocabulary_ attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers’ performance to the point of making them slower than the sequential variant.\n",
    "\n",
    "It is possible to overcome those limitations by combining the “hashing trick” (Feature hashing) implemented by the ```sklearn.feature_extraction.FeatureHasher``` class and the text preprocessing and tokenization features of the ```CountVectorizer```.\n",
    "This combination is implementing in ```HashingVectorizer```, a transformer class that is mostly API compatible with ```CountVectorizer```. ```HashingVectorizer``` is stateless, meaning that you don’t have to call fit on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing vectorizer example:\n",
      "  (0, 144749)\t0.4472135954999579\n",
      "  (0, 170062)\t0.4472135954999579\n",
      "  (0, 286878)\t-0.4472135954999579\n",
      "  (0, 351664)\t-0.4472135954999579\n",
      "  (0, 989160)\t-0.4472135954999579\n",
      "  (1, 144749)\t0.35355339059327373\n",
      "  (1, 170062)\t0.35355339059327373\n",
      "  (1, 286878)\t-0.35355339059327373\n",
      "  (1, 351664)\t-0.35355339059327373\n",
      "  (1, 544379)\t0.7071067811865475\n",
      "  (2, 178949)\t0.5\n",
      "  (2, 180525)\t-0.5\n",
      "  (2, 286878)\t-0.5\n",
      "  (2, 948532)\t-0.5\n",
      "  (3, 144749)\t0.4472135954999579\n",
      "  (3, 170062)\t0.4472135954999579\n",
      "  (3, 286878)\t-0.4472135954999579\n",
      "  (3, 351664)\t-0.4472135954999579\n",
      "  (3, 989160)\t-0.4472135954999579\n",
      "Reducing feature space:\n",
      "  (0, 2)\t0.0\n",
      "  (0, 6)\t-0.5773502691896258\n",
      "  (0, 7)\t0.5773502691896258\n",
      "  (0, 8)\t-0.5773502691896258\n",
      "  (1, 2)\t0.0\n",
      "  (1, 5)\t0.8164965809277261\n",
      "  (1, 7)\t0.4082482904638631\n",
      "  (1, 8)\t-0.4082482904638631\n",
      "  (2, 1)\t0.5\n",
      "  (2, 4)\t-0.5\n",
      "  (2, 5)\t-0.5\n",
      "  (2, 8)\t-0.5\n",
      "  (3, 2)\t0.0\n",
      "  (3, 6)\t-0.5773502691896258\n",
      "  (3, 7)\t0.5773502691896258\n",
      "  (3, 8)\t-0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "print('Hashing vectorizer example:')\n",
    "hv = HashingVectorizer()\n",
    "print(hv.transform(corpus))\n",
    "\n",
    "print('Reducing feature space:')\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "print(hv.transform(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "    The default number of features is 2**18.\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups training data\n",
      "3803 documents - 6.245MB\n",
      "\n",
      "DictVectorizer\n",
      "done in 1.562829s at 3.996MB/s\n",
      "Found 47928 unique terms\n",
      "\n",
      "FeatureHasher on frequency dicts\n",
      "done in 1.117980s at 5.586MB/s\n",
      "Found 100 unique terms\n",
      "\n",
      "FeatureHasher on raw tokens\n",
      "done in 1.093077s at 5.713MB/s\n",
      "Found 100 unique terms\n"
     ]
    }
   ],
   "source": [
    "# Author: Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import DictVectorizer, FeatureHasher\n",
    "\n",
    "\n",
    "def n_nonzero_columns(X):\n",
    "    \"\"\"Returns the number of non-zero columns in a CSR matrix X.\"\"\"\n",
    "    return len(np.unique(X.nonzero()[1]))\n",
    "\n",
    "\n",
    "def tokens(doc):\n",
    "    \"\"\"Extract tokens from doc.\n",
    "\n",
    "    This uses a simple regex to break strings into tokens. For a more\n",
    "    principled approach, see CountVectorizer or TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    return (tok.lower() for tok in re.findall(r\"\\w+\", doc))\n",
    "\n",
    "\n",
    "def token_freqs(doc):\n",
    "    \"\"\"Extract a dict mapping tokens from doc to their frequencies.\"\"\"\n",
    "    freq = defaultdict(int)\n",
    "    for tok in tokens(doc):\n",
    "        freq[tok] += 1\n",
    "    return freq\n",
    "\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'misc.forsale',\n",
    "    'rec.autos',\n",
    "    'sci.space',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following line to use a larger set (11k+ documents)\n",
    "#categories = None\n",
    "\n",
    "print(__doc__)\n",
    "print(\"    The default number of features is 2**18.\")\n",
    "\n",
    "try:\n",
    "    n_features = int(input())\n",
    "except:\n",
    "    n_features = 2 ** 18\n",
    "\n",
    "print(\"Loading 20 newsgroups training data\")\n",
    "raw_data = fetch_20newsgroups(subset='train', categories=categories).data\n",
    "data_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6\n",
    "print(\"%d documents - %0.3fMB\" % (len(raw_data), data_size_mb))\n",
    "print()\n",
    "\n",
    "print(\"DictVectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit_transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
    "print(\"Found %d unique terms\" % len(vectorizer.get_feature_names()))\n",
    "print()\n",
    "\n",
    "print(\"FeatureHasher on frequency dicts\")\n",
    "t0 = time()\n",
    "hasher = FeatureHasher(n_features=n_features)\n",
    "X = hasher.transform(token_freqs(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
    "print(\"Found %d unique terms\" % n_nonzero_columns(X))\n",
    "print()\n",
    "\n",
    "print(\"FeatureHasher on raw tokens\")\n",
    "t0 = time()\n",
    "hasher = FeatureHasher(n_features=n_features, input_type=\"string\")\n",
    "X = hasher.transform(tokens(d) for d in raw_data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_size_mb / duration))\n",
    "print(\"Found %d unique terms\" % n_nonzero_columns(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R channel of imaginary picture\n",
      "[[ 0  3  6  9]\n",
      " [12 15 18 21]\n",
      " [24 27 30 33]\n",
      " [36 39 42 45]]\n",
      "Shape of created patches:\n",
      "(2, 2, 2, 3)\n",
      "R channels of created patches:\n",
      "[[[ 0  3]\n",
      "  [12 15]]\n",
      "\n",
      " [[15 18]\n",
      "  [27 30]]]\n",
      "Shape from another extraction method:\n",
      "(9, 2, 2, 3)\n",
      "Patch 4 (R channel):\n",
      "[[15 18]\n",
      " [27 30]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\n",
    "print('R channel of imaginary picture')\n",
    "print(one_image[:, :, 0])\n",
    "\n",
    "patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2, random_state=0)\n",
    "print('Shape of created patches:')\n",
    "print(patches.shape)\n",
    "print('R channels of created patches:')\n",
    "print(patches[:, :, :, 0])\n",
    "\n",
    "patches = image.extract_patches_2d(one_image, (2, 2))\n",
    "print('Shape from another extraction method:')\n",
    "print(patches.shape)\n",
    "print('Patch 4 (R channel):')\n",
    "print(patches[4, :, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\n",
    "patches = image.PatchExtractor((2, 2)).transform(five_images)\n",
    "print(patches.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of sklearn.preprocessing module\n",
    "The sklearn.preprocessing module has a number of typical transformers, which can be applied on the extracted features. All transformers adhere to the ```Transformer``` API.\n",
    "\n",
    "Let us review a couplle of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data standartization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "Mean:  [0. 0. 0.]\n",
      "Std:  [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(X_scaled)\n",
    "print('Mean: ',X_scaled.mean(axis=0))\n",
    "print('Std: ', X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler mean:  [1.         0.         0.33333333]\n",
      "Scaler std:  [0.81649658 0.81649658 1.24721913]\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[[-2.44948974  1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "print('Scaler mean: ', scaler.mean_)\n",
    "print('Scaler std: ', scaler.scale_)\n",
    "\n",
    "print(scaler.transform(X))\n",
    "print(scaler.transform([[-1.,  1., 0.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a number of different scalers, which can be used.\n",
    "#Check out MinMaxScaler, MaxAbsScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
    "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\n",
    "The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the l1 or l2 norms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "print(X_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n",
      "[[-0.70710678  0.70710678  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "normalizer = preprocessing.Normalizer().fit(X) # fit does nothing\n",
    "print(normalizer.transform(X))\n",
    "print(normalizer.transform([[-1.,  1., 0.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data binarization\n",
    "Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the sklearn.neural_network.BernoulliRBM.\n",
    "It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\n",
    "print(binarizer.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "binarizer = preprocessing.Binarizer(threshold=1.1)\n",
    "print(binarizer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with missing values in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imputer transformer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    "imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    "print(imp.transform(X_test))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial features\n",
    "Often it’s useful to add complexity to the model by considering nonlinear features of the input data. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[ 1.  0.  1.  0.  0.  1.]\n",
      " [ 1.  2.  3.  4.  6.  9.]\n",
      " [ 1.  4.  5. 16. 20. 25.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "print(X)\n",
    "poly = PolynomialFeatures(2)\n",
    "results = poly.fit_transform(X)                             \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom transformers\n",
    "Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.69314718]\n",
      " [1.09861229 1.38629436]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "print(transformer.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining data analysis tasks with sklearn.pipeline\n",
    "sklearn.pipeline allows you to chain your data analysis task and steps. Basically, it implements a chain of responsibility pattern. \n",
    "\n",
    "Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves two purposes here:\n",
    "\n",
    "* Convenience: You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "* Joint parameter selection: You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -1.22474487 -0.98058068  1.5         1.20096115  0.96153846]\n",
      " [ 1.          0.         -0.39223227  0.         -0.          0.15384615]\n",
      " [ 1.          1.22474487  1.37281295  1.5         1.68134561  1.88461538]]\n",
      "[[ 1.          0.         -0.98058068  0.         -0.          0.96153846]\n",
      " [ 1.          0.81649658  0.          0.66666667  0.          0.        ]\n",
      " [ 1.          1.22474487  1.37281295  1.5         1.68134561  1.88461538]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\utils\\deprecation.py:58: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "imputer_transform = ('fill_NaNs', Imputer(missing_values='NaN', strategy='mean', axis=0))\n",
    "scaler_transform = ('scale', StandardScaler())\n",
    "polynomial_features_transform = ('polynomial_features', PolynomialFeatures())\n",
    "\n",
    "pipeline = Pipeline(steps = [imputer_transform,\n",
    "                             scaler_transform,\n",
    "                             polynomial_features_transform]\n",
    "                   )\n",
    "X_train = [[1, 2], [np.nan, 3], [7, 6]]\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "print(pipeline.transform(X_train))\n",
    "\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(pipeline.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline_test.py\n"
     ]
    }
   ],
   "source": [
    "%%file pipeline_test.py\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n",
    "###############################################################################\n",
    "# define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module created for script run in IPython\n",
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 20.140s\n",
      "\n",
      "Best score: 0.938\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\miklesh\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%run pipeline_test.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
